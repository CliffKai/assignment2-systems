import subprocess
import csv
import os
import re
import argparse
import datetime
import sys

# ================= é…ç½®åŒºåŸŸ =================
# ä½ å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´è¦æµ‹è¯•çš„å‚æ•°èŒƒå›´
MODEL_SIZES = ["small", "medium", "large", "xl", "2.7B"]
MODES = ["fwd", "bwd"]
PRECISIONS = ["fp32", "bf16"] # bf16 å¯¹åº” --mixed_precision
CONTEXT_LENGTH = 128
# ===========================================

def parse_output(output_str):
    """ä» benchmark.py çš„è¾“å‡ºä¸­æå– Avg Time å’Œ Std Dev"""
    avg_match = re.search(r"Avg Time:\s+([\d\.]+)\s+ms", output_str)
    std_match = re.search(r"Std Dev:\s+([\d\.]+)\s+ms", output_str)
    
    avg_time = float(avg_match.group(1)) if avg_match else None
    std_dev = float(std_match.group(1)) if std_match else None
    return avg_time, std_dev

def main():
    parser = argparse.ArgumentParser(description="Batch runner for CS336 benchmarks")
    parser.add_argument("--device", type=str, default="0", help="CUDA device ID")
    parser.add_argument("--repeats", type=int, default=3, help="Number of times to run each config")
    parser.add_argument("--steps", type=int, default=100, help="Number of steps inside benchmark.py")
    parser.add_argument("--warmup_steps", type=int, default=50, help="Number of warmup steps inside benchmark.py")
    args = parser.parse_args()

    # 1. å‡†å¤‡ç»“æœç›®å½•å’Œæ–‡ä»¶
    # å½“å‰åœ¨ cs336_systems, ç›®æ ‡æ˜¯ ../result
    current_dir = os.path.dirname(os.path.abspath(__file__))
    result_dir = os.path.join(os.path.dirname(current_dir), "result")
    os.makedirs(result_dir, exist_ok=True)
    
    csv_file = os.path.join(
        result_dir,
        f"benchmark_w{args.warmup_steps}_s{args.steps}_r{args.repeats}.csv"
    )
    file_exists = os.path.isfile(csv_file)

    print(f"ğŸš€ Starting Batch Benchmark on GPU {args.device}...")
    print(f"ğŸ“‚ Results will be saved to: {csv_file}")
    print("-" * 60)

    # 2. æ‰“å¼€ CSV æ–‡ä»¶ (è¿½åŠ æ¨¡å¼ 'a')
    with open(csv_file, mode='a', newline='') as f:
        writer = csv.writer(f)
        
        # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
        if not file_exists:
            header = [
                "Timestamp",
                "Model_Size",
                "Mode",
                "Precision",
                "Context_Len",
                "Run_Index",
                "Avg_Time_ms",
                "Std_Dev_ms",
                "Steps_Per_Run",
                "Warmup_Steps"
            ]
            writer.writerow(header)

        # 3. å¾ªç¯éå†æ‰€æœ‰é…ç½®
        total_experiments = len(MODEL_SIZES) * len(MODES) * len(PRECISIONS) * args.repeats
        count = 0

        for size in MODEL_SIZES:
            for mode in MODES:
                for prec in PRECISIONS:
                    # æ„å»ºåŸºç¡€å‘½ä»¤
                    cmd = [
                        "uv", "run", "python", "benchmark.py",
                        "--model_size", size,
                        "--mode", mode,
                        "--context_length", str(CONTEXT_LENGTH),
                        "--steps", str(args.steps),
                        "--warmup_steps", str(args.warmup_steps),
                    ]
                    
                    if prec == "bf16":
                        cmd.append("--mixed_precision")

                    # é‡å¤è¿è¡Œ N æ¬¡
                    for i in range(args.repeats):
                        count += 1
                        print(f"[{count}/{total_experiments}] Running: {size} | {mode} | {prec} | Run {i+1}/{args.repeats} ... ", end="", flush=True)
                        
                        try:
                            # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å®š GPU
                            env = os.environ.copy()
                            env["CUDA_VISIBLE_DEVICES"] = args.device

                            # æ‰§è¡Œå‘½ä»¤
                            result = subprocess.run(
                                cmd, 
                                env=env, 
                                capture_output=True, 
                                text=True,
                                check=True
                            )
                            
                            # è§£æç»“æœ
                            avg_time, std_dev = parse_output(result.stdout)
                            
                            if avg_time is not None:
                                timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                writer.writerow([timestamp, size, mode, prec, CONTEXT_LENGTH, i+1, avg_time, std_dev, args.steps])
                                f.flush() # ç«‹å³å†™å…¥ç£ç›˜ï¼Œé˜²æ­¢ä¸­æ–­ä¸¢å¤±
                                print(f"âœ… {avg_time:.2f} ms")
                            else:
                                print("âš ï¸ Parse Error (Check benchmark.py output)")

                        except subprocess.CalledProcessError as e:
                            print(f"âŒ Failed (OOM or Error)")
                            # å¯ä»¥é€‰æ‹©è®°å½•é”™è¯¯åˆ° CSVï¼Œæˆ–è€…ç›´æ¥è·³è¿‡
                            # writer.writerow([timestamp, size, mode, prec, CONTEXT_LENGTH, i+1, "ERROR", "ERROR", args.steps])

    print("-" * 60)
    print("ğŸ‰ All benchmarks completed!")

if __name__ == "__main__":
    main()